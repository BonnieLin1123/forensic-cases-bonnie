# Bias in Facial Recognition Used in Forensic Investigations

## Intro
Facial Recognition (FR) Algorithm is a powerful tool and is significant in law enforcement to assist in identifying suspects, making the arrest process more efficient. However, using the FR technology effectively requires appropriate training and awareness of potential bias to optimize the functionality of the tools. Otherwise, improper use can lead to serious consequences such as risking an innocent person’s dignity and freedom. This paper discusses how the inappropriate use and imperfect of FR systems can be ethically and practically unacceptable by applying consequentialism and deontology to evaluate their harms and moral failings. Consequentialism is an ethical framework that evaluates the morality of an action based on its outcomes, while deontology asserts that ethical decisions should be guided by moral duties to protect humans, regardless of the consequences. Under these frameworks, we will analyze the potential biases involved during the forensic investigation, evaluate ethical implications of the biases, and provide solutions to minimize the biases.

## Williams v. Detroit
To begin, we will explore the case of Williams v. Detroit to understand how implicit and explicit biases are present in the process of forensic investigations that result in serious injustice. The Detroit Police Department (DPD) was investigating a watch theft from a luxury store. The DPD obtained a blurry, low-quality image from the store’s surveillance video and took it to process a face recognition technology search by a company called DataWorks Plus. The photo of Robert Williams, a black man, appeared to be a possible match with the suspect from the surveillance footage. The DPD applied for an arrest warrant without conducting any further investigation. He was detained for 30 hours without being provided food and water. However, the County announced Williams could be released due to insufficient evidence. After that, Williams sued the department for his wrongful arrest and sought to have the technology banned. DataWorks Plus argues that the company does not instruct law enforcement agencies on how to use the software and that it does not output a single definite match, but rather returns a list of many possible candidates with similarity scores. This incident sparked public concerns about the accuracy and fairness of FR systems and prompted us to evaluate whether implicit biases in the data training and law enforcement databases along with explicit bias in the human application of the technology can lead to serious ethical failures. 

## Possible Implicit Bias I
A possible implicit bias occurs since the accuracy of a FR system heavily depends on the quality and diversity of training data. If the training data contains fewer images of Black, Asian, or Indigenous individuals, the algorithm will become less accurate at recognizing those faces. As a result, representation bias sets in since the algorithm cannot accurately serve all population it is supposed to. The U.S. National Institute of Standards and Technology (NIST) reported that many FR algorithms were 10 to 100 times more likely to generate a false match for a Black or East Asian face compared to a white face​. The training data from DataWorks were likely dominated by white faces which is why it performs poorly on non-white communities. DataWorks’ general manager has acknowledged that the company only relies on vendor claims and basic tests, which means searching for known individuals with low-quality photos and testing if the algorithm could output the correct individual, rather than doing rigorous scientific evaluations​. This lack of proactive bias auditing means that any skew in the training data may go undetected until real-world failures occur. From a consequentialist standpoint, the inaccuracy can lead to false arrests of innocent people and increase public distrust in technology and law enforcement. The algorithm could be viewed as a tool for oppressing non-white communities, resulting in greater societal division and skepticism toward the use of AI technology. The erosion of trust from the public and institutions leads to stagnation of innovating and improving fair AI algorithms to serve all communities. From a deontological view, justice systems should be impartial and treat every individual equally. Yet, biased AI systems disproportionately misidentify people of color, and the algorithm violates the duty to treat all people without discrimination. To address the database's inherent bias, the government should enforce regular independent testing of any demographic bias for algorithms used in law enforcement. If an algorithm is found to have high error rates for a certain group, it should undergo technical improvements and be restricted to use before the bias is solved. By doing this, we can ensure the published algorithms are unbiased and could serve the full diversity of the population equally.

## Possible Implicit Bias II
Another implicit bias happens when law enforcement databases contain more black and brown individuals due to historical over-policing. The law enforcement databases have overrepresented colored communities not because they committed more crimes, but because they are surveilled more heavily. As a result, even when the correct person to investigate was not in the database, the algorithm would still return a list of false match candidates that are skewed towards these overrepresented groups. This has led to compounding bias which occurs when the system keeps using the same disproportionated data and results in a more biased data that gets fed back to the system, making the bias in the database worsen over time. This creates a feedback loop where the use of biased data reinforces existing racial disparities in arrests and criminal records. From a consequentialist viewpoint, this cycle contributes to systemic discrimination and exacerbates racial inequalities. It will be more challenging for over-policed communities to gain respect and advocate for equality when many of them have incorrect records of involvement in criminal cases. Racism and discrimination will continue to negatively impact the life experiences of people of color. A deontologist would argue that disproportionately targeting specific communities violates the principle of equal protection under the law. It is our duty to treat every individual equally, regardless of their race. To mitigate the compounding and structural bias, officers should regularly update the databases to counter historical injustices and over-policing. FR results must also be reviewed by trained human analysts and should be treated only as investigative leads, not definitive proof. Also, the defendants must have the right to be notified when the technology has been used in their investigations. Doing these prevents our law enforcement from falling into the vicious cycle of using incorrect datasets and allows the defendant to protect their rights when FR technology is put in use which helps to preserve public trust in AI technology.

## Explicit Biases
Besides the implicit biases from the algorithm and databases, human-made explicit biases could worsen the problem of misidentification that already existed in the FR systems. The investigators might ignore the fact that surveillance photos being processed to FR algorithms often cannot accurately present the suspect because of the images’ poor quality. When the fed-in images are blurry or only show partial features of the suspect, the algorithm’s output is likely to be unreliable. Although some algorithms allow manual adjustments to the picture, such as adjusting its brightness or rotating images, this raises concerns that the edited image could alter the important facial features of the person. However, the system will still return a list of possible candidates based on the edited image. If the officers unconsciously assume the result of the algorithm is correct, which they only focus on the similarities of the matches with the input photos rather than the differences, a confirmation bias sets in. From a consequentialist standpoint, this could lead to the wrongful arrest of an innocent person, who may experience inhumane treatment during the arrest and be traumatized about the experience. For example, Williams was not given any food or water for 30 hours during the investigation. The innocent individuals are forced to experience emotional and psychological despair. Deontologically, the reliance on edited images in law enforcement violates the police’s duty to protect the rights of every individual. It is immoral to put an innocent individual’s freedom at risk by using non-standardized images. For resolution, investigators should undergo comprehensive training before they can use the FR algorithms and set limitations or  formalize a confidence threshold based on how clearly the suspect is presented. If not pass the training or the confidence threshold, the police should never use the FR algorithm at any point. By adding restrictions, we can lower the manual bias in the process of finding suspicious criminals.

## Conclusion
The Williams v. Detroit case exposes the wrongful arrest was due to the implicit biases in the algorithm's process of training data and the over-representation of certain communities in law enforcement databases, as well as explicit bias in how humans utilize the FR algorithms in forensic investigations. We used both consequentialism and deontology ethical frameworks to prove that relying on the algorithm in investigations is morally and operationally unjustifiable. Consequentialism reveals the harm caused by biased algorithms, such as traumatizing innocent individuals, eroding public trust in AI algorithms, and perpetuating societal divisions. Deontology reminds us of our moral duty to treat all individuals equally and protect their rights regardless of technological convenience. These ethical perspectives guided our solutions to mitigate the biases from facial algorithms, complete rigorous bias tests before publishing the algorithms, constantly update the law enforcement datasets, and minimize human bias setting restrictions. Our solutions ensure that every algorithm used for decision-making is carefully examined to serve justice as we should not put anyone's dignity or freedom at risk.

## Works Cited
https://www.aclu.org/news/privacy-technology/i-did-nothing-wrong-i-was-arrested-anyway

https://www.technologyreview.com/2021/04/14/1022676/robert-williams-facial-recognition-lawsuit-aclu-detroit-police/

https://www.techdirt.com/2024/07/03/detroit-alters-facial-recognition-use-rules-in-response-to-multiple-bogus-arrests/

https://www.aclu.org/cases/williams-v-city-of-detroit-face-recognition-false-arrest

https://www.nist.gov/programs-projects/face-recognition-vendor-test-frvt

https://www.nist.gov/news-events/news/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software

https://www.pnas.org/doi/10.1073/pnas.1721355115

https://learnopencv.com/face-recognition-models/

